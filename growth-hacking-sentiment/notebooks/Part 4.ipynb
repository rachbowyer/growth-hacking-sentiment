{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e01186-f8a8-4359-8d04-de60499b5946",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "\n",
    "The code for part 4 is in the Python scripts in `../part-4/neural_networks.py`\n",
    "\n",
    "## The build up to Part 4\n",
    "\n",
    "1. Part-1 was concerned with data prep and imbalanced datasets\n",
    "2. Part-2 created a dictionary based classifier, tried different correlation measures and added code to handle negation\n",
    "3. Part-3 focussed on the evaluation of the dictionary based evaluator. Ratings got converted into three ratings classes using supplied thresholds. The dictionary based classifier was also converted to give 3 rating classes using supplied thresholds. Then accuracy, precision, recall, F1 score and confusion matrix were calculated.\n",
    "\n",
    "\n",
    "## Summary of the work in Part 4\n",
    "\n",
    "Part-4 covered creating and evaluating a series of models.\n",
    "\n",
    "* The dictionary based model built in Part 3 was used as a base line\n",
    "* A pretrained classifier based on DistilBERT from the HuggingFaces pipelines package\n",
    "* A classifier trained on the supervised Games review data based on RoBERTa from the Simple Transformers package\n",
    "* A classified based on fine tuning the RoBERTa model on the unsupervised Games review data before training a classifier using the supervised Games review data\n",
    "\n",
    "\n",
    "## Dictionary based classifier\n",
    "\n",
    "The metrics for the Dictionary based classifier from Part 3 are:\n",
    "\n",
    "```\n",
    "Accuracy score: 0.39\n",
    "Precision score: 0.64\n",
    "Recall score: 0.39\n",
    "\n",
    "\n",
    "Confusion matrix\n",
    "  \n",
    " Actual negative  [[  49 1442    9]\n",
    " Actual neutral    [  16 1421   63]\n",
    " Actual positive   [   0 1201  299]]\n",
    "```\n",
    "\n",
    "\n",
    "## Hugging Faces pretrained dictionary bassed classifier\n",
    "\n",
    "* The Hugging Faces pretrained \"pipeline' classifier was used based on the `distilbert-base-uncased-finetuned-sst-2-english'` model.\n",
    "* Thresholds were adjusted to deliver the best results on the evaluation set.\n",
    "* Thresholds chosen negative >0.998, positive >0.94\n",
    "\n",
    "```\n",
    " Accuracy score: 0.64\n",
    " Precision score: 0.62\n",
    " Recall score: 0.64\n",
    "\n",
    " Confusion matrix\n",
    "  Actual negative  [[1018  415   67]\n",
    "  Actual neutral   [ 395  570  535]\n",
    "  Actual positive  [  32  198 1270]]\n",
    "\n",
    "\n",
    " Finished eval_model1() in 807.2092 secs\n",
    " Finished processing reviews.\n",
    "```\n",
    "\n",
    "* This has lead to a massive improvement in the results, in particular accuracy and recall\n",
    "\n",
    "\n",
    "\n",
    "### Classifier based on the RoBERTa model\n",
    "\n",
    "* A classifier based on the Simple Transformers RoBERTa base model trained on the supervised Games Review data.\n",
    "* roberta-base\n",
    "* max_seq_length: 512\n",
    "* sliding_window: True\n",
    "* num_train_epochs: 1\n",
    "* train_batch_size: 20\n",
    "* training set: 2250\n",
    "\n",
    "The results\n",
    "\n",
    "```\n",
    "\n",
    "Roberta-base 512k sequences\n",
    "Finished create_model2() in 3592.0646 secs - 59 minutes\n",
    "Accuracy score: 0.77\n",
    "Precision score: 0.76\n",
    "Recall score: 0.77\n",
    "\n",
    "\n",
    "Confusion matrix\n",
    "Actual negative   [[646  86  18]\n",
    "Actual neutral     [183 428 139]\n",
    "Actual positive    [  5  95 650]] \n",
    "\n",
    "\n",
    "Finished create_model2() in 3592.0646 secs\n",
    "Finished create_model2() in 3774.2609 secs with device = 'MPS'\n",
    "\n",
    "\n",
    "Finished eval_model_2() in 1885.6078 secs - Need to double check this?\n",
    "\n",
    "```\n",
    "\n",
    "* Accuracy, precision and recall have improved significantly over the pre-trained classifier\n",
    "* This is potentially due to the model learning hower 'Gamer Speak' affects the rating. For example, any review that mentions 'DRM' is likely to be negative.\n",
    "\n",
    "\n",
    "\n",
    "### Classifier based on a fine tuned RoBERTa model\n",
    "\n",
    "\n",
    "* roberta-base\n",
    "* max_seq_length: 512\n",
    "* sliding_window: True\n",
    "* train_batch_size: 20\n",
    "* num_train_epochs: 1\n",
    "* Unsupervised data was just the reviews from the Training set\n",
    "\n",
    "For the classifier\n",
    "* max_seq_length: 512\n",
    "* sliding_window: True\n",
    "* train_batch_size: 20\n",
    "* num_train_epochs: 1\n",
    "* training set: 2250\n",
    "\n",
    "\n",
    "The results\n",
    "\n",
    "\n",
    "```\n",
    "# Accuracy score: 0.75\n",
    "# Precision score: 0.75\n",
    "# Recall score: 0.75\n",
    "#\n",
    "# Confusion matrix\n",
    "# [[658  71  21]\n",
    "#  [210 375 165]\n",
    "#  [ 10  77 663]]\n",
    "```\n",
    "\n",
    "* Fine tuning the model has made no improvement, in fact it has made the model worse. However, the difference in performance between the two models is too small to be statistically significant.\n",
    "* The instructions for the task are ambiguious. I have used the same training set that I used for the classification for the fine tuning. As the model is getting no new information, this may explain why performance has not improved. My preference would be to use a lot of additional data for the fine tuning.\n",
    "* \\[Update: have peeked at the model solution. My results using RoBERTa are significantly better than the model solution. Also the model solution was not improved by fine tuning. Further, the model solution also used the same data to both fine tune and train the classifier\\]\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* Overall the best performance came from a trained classifier (without fine tuning) based on a RoBERTa model.\n",
    "* Performance is massively improved compared with the naive dictionary based sentiment classifier. This reflects the far greater symantic understanding achived by the RoBERTa model\n",
    "* The GPT 3.x and 4.x models were not available when this project was created. It would be really interesting to see how well they perform.\n",
    "* Finally, a success rate of 100% is unlikely to be possible. People do not always behave rationally. A review might say how they dislike a game, but still give it 5 stars.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
