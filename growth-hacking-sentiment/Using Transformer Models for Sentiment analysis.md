# Using transformer models for NLP

Transformer models are trained using unsupervised learning with large amounts of text. There are 3 main techniques:

1. Next Sentence Prediction (NSP). Here the model is given one snetenc and trained to predict the next sentence.

2. Masked language modelling (MLM). Here a single word in a sentence is masked and the model is trained to predict the word that is masked from the context.

3. ELECTRA. This a more efficient (in terms of compute) version of MLM. There are a pair of models (similar to a GAN) - the generator and discriminator. Both models are transformer models. The generator is trained for a few rounds to produce plausible replacements for words in the text. Then the discriminator is trained to distinguish beteween the orginal word and the replacement. (Note if the generator predicts the correct word, then this is scored as the correct word.) 


## BERT

Is an open source model from Google introduced in 2018. In 2019 Google started to use it in their search engine. BERT stands for Bidirectional Encoder Representations from Transformers. It was trained on English Wikipedia and the Brown Corpus using NSP and MLM techniques. It is bidirectional in that it takes into account words before and after the word it is trying to predict.


## DistilBERT

This is a smaller version of BERT. It was created by using the distillation technique. Basically a new model with fewer parameters is created using output generated by BERT. The model needs less memory and answers questions faster.


## RoBERTa

This is a successor model to BERT, using the same architecture. It improves training by removing NSP and using different hyper parameters.



## Using transfer learning to solve NLP tasks

DistilBERT provide a great based for training models to solve NLP tasks. So to measure the sentiment in a piece of text, you would add new output nodes culminating in a sigmoid or similar. Weights in the base layer are frozen. The model is then trained with examples.



## Fine tuning a LLM

A LLM can be fine tuned to be better at working with your specieic vocabulary - e.g. a specifice domain. Here large amounts of unlabelled data are fed into the model and all the weights are adjusted.






